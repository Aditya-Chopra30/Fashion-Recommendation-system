{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgxzHI-R3Y4P"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gdown\n",
        "from fastai.vision import *\n",
        "from fastai.metrics import accuracy\n",
        "from fastai.metrics import top_k_accuracy\n",
        "from annoy import AnnoyIndex\n",
        "import zipfile\n",
        "import time\n",
        "from google.colab import drive\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the meta data\n",
        "url = 'https://drive.google.com/uc?id=0B7EVK8r0v71pWnFiNlNGTVloLUk'\n",
        "output = 'list_category_cloth.txt'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=0B7EVK8r0v71pTGNoWkhZeVpzbFk'\n",
        "output = 'list_category_img.txt'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=0B7EVK8r0v71pdS1FMlNreEwtc1E'\n",
        "output = 'list_eval_partition.txt'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "4CM9Aasu3nI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the images\n",
        "root_path = './'\n",
        "url = 'https://drive.google.com/uc?id=1j5fCPgh0gnY6v7ChkWlgnnHH6unxuAbb'\n",
        "output = 'img.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "with zipfile.ZipFile(\"img.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(root_path)"
      ],
      "metadata": {
        "id": "yZpeYIWI3qoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# category names\n",
        "with open('list_category_cloth.txt', 'r') as f:\n",
        "    for i, line in enumerate(f.readlines()):\n",
        "        if i &gt; 1:\n",
        "            category_list.append(line.split(' ')[0])\n",
        "\n",
        "# category map\n",
        "with open('list_category_img.txt', 'r') as f:\n",
        "    for i, line in enumerate(f.readlines()):\n",
        "        if i &gt; 1:\n",
        "            image_path_list.append([word.strip() for word in line.split(' ') if len(word) &gt; 0])\n",
        "\n",
        "\n",
        "# train, valid, test\n",
        "with open('list_eval_partition.txt', 'r') as f:\n",
        "    for i, line in enumerate(f.readlines()):\n",
        "        if i &gt; 1:\n",
        "            data_type_list.append([word.strip() for word in line.split(' ') if len(word) &gt; 0])\n",
        "\n",
        "data_df = pd.DataFrame(image_path_list, columns=['image_path', 'category_number'])\n",
        "data_df['category_number'] = data_df['category_number'].astype(int)\n",
        "data_df = data_df.merge(pd.DataFrame(data_type_list, columns=['image_path', 'dataset_type']), on='image_path')\n",
        "data_df['category'] = data_df['category_number'].apply(lambda x: category_list[int(x) - 1])\n",
        "data_df = data_df.drop('category_number', axis=1)  "
      ],
      "metadata": {
        "id": "P9WsIp7w3vNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_list = ImageList.from_df(df=data_df, path=root_path, cols='image_path').split_by_idxs(\n",
        "    (data_df[data_df['dataset_type']=='train'].index),\n",
        "    (data_df[data_df['dataset_type']=='val'].index)).label_from_df(cols='category')\n",
        "test_image_list = ImageList.from_df(df=data_df[data_df['dataset_type'] == 'test'], path=root_path, cols='image_path')\n",
        "\n",
        "data = train_image_list.transform(get_transforms(), size=224).databunch(bs=128).normalize(imagenet_stats)\n",
        "data.add_test(test_image_list)\n",
        "data.show_batch(rows=3, figsize=(8,8))"
      ],
      "metadata": {
        "id": "BdsH9qfM31Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see models available: https://docs.fast.ai/vision.models.html\n",
        "# many options for Resnet, the numbers are the number of layers. \n",
        "# More layers are generally more accurate but take longer to train: resnet18, resnet34, resnet50, resnet101, resnet152\n",
        "# get top 1 and top 5 accuracy\n",
        "def train_model(data, pretrained_model, model_metrics):\n",
        "    learner = cnn_learner(data, pretrained_model, metrics=model_metrics)\n",
        "    learner.model = torch.nn.DataParallel(learner.model)\n",
        "    learner.lr_find()\n",
        "    learner.recorder.plot(suggestion=True)\n",
        "    return learner\n",
        "\n",
        "pretrained_model = models.resnet18 # simple model that can be trained on free tier\n",
        "# pretrained_model = models.resnet50 # need pro tier, model I used\n",
        "\n",
        "model_metrics = [accuracy, partial(top_k_accuracy, k=1), partial(top_k_accuracy, k=5)]\n",
        "learner = train_model(data, pretrained_model, model_metrics)\n",
        "learner.fit_one_cycle(10, max_lr=1e-02)"
      ],
      "metadata": {
        "id": "6YdldUSC35Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interp = ClassificationInterpretation.from_learner(learner)\n",
        "interp.plot_top_losses(9, largest=False, figsize=(15,11), heatmap_thresh=5)"
      ],
      "metadata": {
        "id": "BlDnMuU83-ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model (temporary, will lose model once environment resets)\n",
        "learner.save('resnet-fashion')"
      ],
      "metadata": {
        "id": "R44t8XWQ4B1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SaveFeatures():\n",
        "    features=None\n",
        "    def __init__(self, m): \n",
        "        self.hook = m.register_forward_hook(self.hook_fn)\n",
        "        self.features = None\n",
        "    def hook_fn(self, module, input, output): \n",
        "        out = output.detach().cpu().numpy()\n",
        "        if isinstance(self.features, type(None)):\n",
        "            self.features = out\n",
        "        else:\n",
        "            self.features = np.row_stack((self.features, out))\n",
        "    def remove(self): \n",
        "        self.hook.remove()\n",
        "   \n",
        "  # load the trained model\n",
        "def load_learner(data, pretrained_model, model_metrics, model_path):\n",
        "    learner = cnn_learner(data, pretrained_model, metrics=model_metrics)\n",
        "    learner.model = torch.nn.DataParallel(learner.model)\n",
        "    learner = learner.load(model_path)\n",
        "    return learner\n",
        "\n",
        "pretrained_model = models.resnet18 # simple model that can be trained on free tier\n",
        "# pretrained_model = models.resnet50 # need pro tier\n",
        "\n",
        "model_metrics = [accuracy, partial(top_k_accuracy, k=1), partial(top_k_accuracy, k=5)]\n",
        "# if gdrive not mounted:\n",
        "drive.mount('/content/gdrive') \n",
        "\n",
        "\n",
        "model_path = \"/content/gdrive/My Drive/resnet18-fashion\"\n",
        "# model_path = \"/content/gdrive/My Drive/resnet50-fashion\"\n",
        "learner = load_learner(data, pretrained_model, model_metrics, model_path)"
      ],
      "metadata": {
        "id": "s_zk-Dpc4Fdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes time to populate the embeddings for each image\n",
        "# Get 2nd last layer of the model that stores the embedding for the image representations\n",
        "# the last linear layer is the output layer.\n",
        "saved_features = SaveFeatures(learner.model.module[1][4])\n",
        "_= learner.get_preds(data.train_ds)\n",
        "_= learner.get_preds(DatasetType.Valid)"
      ],
      "metadata": {
        "id": "505cQL_V4JTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the data for generating recommendations (exlcude test data)\n",
        "# get the embeddings from trained model\n",
        "img_path = [str(x) for x in (list(data.train_ds.items) +list(data.valid_ds.items))]\n",
        "label = [data.classes[x] for x in (list(data.train_ds.y.items) +list(data.valid_ds.y.items))]\n",
        "label_id = [x for x in (list(data.train_ds.y.items) +list(data.valid_ds.y.items))]\n",
        "data_df_ouput = pd.DataFrame({'img_path': img_path, 'label': label, 'label_id': label_id})\n",
        "data_df_ouput['embeddings'] = np.array(saved_features.features).tolist()\n",
        "# Using Spotify's Annoy\n",
        "def get_similar_images_annoy(annoy_tree, img_index, number_of_items=12):\n",
        "    start = time.time()\n",
        "    img_id, img_label  = data_df_ouput.iloc[img_index, [0, 1]]\n",
        "    similar_img_ids = annoy_tree.get_nns_by_item(img_index, number_of_items+1)\n",
        "    end = time.time()\n",
        "    print(f'{(end - start) * 1000} ms')\n",
        "    # ignore first item as it is always target image\n",
        "    return img_id, img_label, data_df_ouput.iloc[similar_img_ids[1:]] \n",
        "\n",
        "\n",
        "# for images similar to centroid \n",
        "def get_similar_images_annoy_centroid(annoy_tree, vector_value, number_of_items=12):\n",
        "    start = time.time()\n",
        "    similar_img_ids = annoy_tree.get_nns_by_vector(vector_value, number_of_items+1)\n",
        "    end = time.time()\n",
        "    print(f'{(end - start) * 1000} ms')\n",
        "    # ignore first item as it is always target image\n",
        "    return data_df_ouput.iloc[similar_img_ids[1:]] \n",
        "\n",
        "\n",
        "def show_similar_images(similar_images_df, fig_size=[10,10], hide_labels=True):\n",
        "    if hide_labels:\n",
        "        category_list = []\n",
        "        for i in range(len(similar_images_df)):\n",
        "            # replace category with blank so it wont show in display\n",
        "            category_list.append(CategoryList(similar_images_df['label_id'].values*0,\n",
        "                                              [''] * len(similar_images_df)).get(i))\n",
        "    else:\n",
        "        category_list = [learner.data.train_ds.y.reconstruct(y) for y in similar_images_df['label_id']]\n",
        "    return learner.data.show_xys([open_image(img_id) for img_id in similar_images_df['img_path']],\n",
        "                                category_list, figsize=fig_size)\n",
        "  # more tree = better approximation\n",
        "ntree = 100\n",
        "#\"angular\", \"euclidean\", \"manhattan\", \"hamming\", or \"dot\"\n",
        "metric_choice = 'angular'\n",
        "\n",
        "annoy_tree = AnnoyIndex(len(data_df_ouput['embeddings'][0]), metric=metric_choice)\n",
        "\n",
        "# # takes a while to build the tree\n",
        "for i, vector in enumerate(data_df_ouput['embeddings']):\n",
        "    annoy_tree.add_item(i, vector)\n",
        "_  = annoy_tree.build(ntree)"
      ],
      "metadata": {
        "id": "9h7pWN3q4M9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def centroid_embedding(outfit_embedding_list):\n",
        "    number_of_outfits = outfit_embedding_list.shape[0]\n",
        "    length_of_embedding = outfit_embedding_list.shape[1]\n",
        "    centroid = []\n",
        "    for i in range(length_of_embedding):\n",
        "        centroid.append(np.sum(outfit_embedding_list[:, i])/number_of_outfits)\n",
        "    return centroid\n",
        " # shorts\n",
        "outfit_img_ids = [109938, 106385, 113703, 98666, 113467, 120667, 20840, 8450, 142843, 238607, 124505,222671]\n",
        "outfit_embedding_list = []\n",
        "for img_index in outfit_img_ids:\n",
        "    outfit_embedding_list.append(data_df_ouput.iloc[img_index, 3])\n",
        "\n",
        "outfit_embedding_list = np.array(outfit_embedding_list)\n",
        "outfit_centroid_embedding = centroid_embedding(outfit_embedding_list)\n",
        "outfits_selected = data_df_ouput.iloc[outfit_img_ids] \n",
        "\n",
        "similar_images_df = get_similar_images_annoy_centroid(annoy_tree, outfit_centroid_embedding, 30)"
      ],
      "metadata": {
        "id": "VdsJkQmC4Rgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_similar_images(outfits_selected, fig_size=[15,15])"
      ],
      "metadata": {
        "id": "EyFKUYmc4VjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qFLRhxBs4YP4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}